{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948c868c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yellowbrick in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (1.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from yellowbrick) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from yellowbrick) (1.0.2)\n",
      "Requirement already satisfied: cycler>=0.10.0 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from yellowbrick) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from yellowbrick) (1.20.3)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from yellowbrick) (3.5.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\diron\\anaconda3\\envs\\mlenv\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aed3644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from pathlib import Path\n",
    "from scipy.stats import probplot, chi2_contingency, chi2\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, roc_curve, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import scipy.stats as stats\n",
    "import scikitplot as skplt\n",
    "import joblib\n",
    "import os\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05383e41",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'application_record.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15764/1612130232.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcc_data_full_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'application_record.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcredit_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'credit_record.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m             )\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'application_record.csv'"
     ]
    }
   ],
   "source": [
    "cc_data_full_data = pd.read_csv('application_record.csv')\n",
    "credit_status = pd.read_csv('credit_record.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ad0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 creating the target variable\n",
    "begin_month=pd.DataFrame(credit_status.groupby(['ID'])['MONTHS_BALANCE'].agg(min))\n",
    "begin_month=begin_month.rename(columns={'MONTHS_BALANCE':'Account age'})\n",
    "cc_data_full_data=pd.merge(cc_data_full_data,begin_month,how='left',on='ID')\n",
    "credit_status['dep_value'] = None\n",
    "credit_status['dep_value'][credit_status['STATUS'] =='2']='Yes'\n",
    "credit_status['dep_value'][credit_status['STATUS'] =='3']='Yes'\n",
    "credit_status['dep_value'][credit_status['STATUS'] =='4']='Yes'\n",
    "credit_status['dep_value'][credit_status['STATUS'] =='5']='Yes'\n",
    "cpunt=credit_status.groupby('ID').count()\n",
    "cpunt['dep_value'][cpunt['dep_value'] > 0]='Yes'\n",
    "cpunt['dep_value'][cpunt['dep_value'] == 0]='No'\n",
    "cpunt = cpunt[['dep_value']]\n",
    "cc_data_full_data = pd.merge(cc_data_full_data,cpunt,how='inner',on='ID')\n",
    "cc_data_full_data['Is high risk']=cc_data_full_data['dep_value']\n",
    "cc_data_full_data.loc[cc_data_full_data['Is high risk']=='Yes','Is high risk']=1\n",
    "cc_data_full_data.loc[cc_data_full_data['Is high risk']=='No','Is high risk']=0\n",
    "cc_data_full_data.drop('dep_value',axis=1,inplace=True)\n",
    "warnings.simplefilter(action='always', category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the features to a more readable feature names\n",
    "cc_data_full_data = cc_data_full_data.rename(columns={\n",
    "    'CODE_GENDER':'Gender',\n",
    "    'FLAG_OWN_CAR':'Has a car',\n",
    "    'FLAG_OWN_REALTY':'Has a property',\n",
    "    'CNT_CHILDREN':'Children count',\n",
    "    'AMT_INCOME_TOTAL':'Income',\n",
    "    'NAME_INCOME_TYPE':'Employment status',\n",
    "    'NAME_EDUCATION_TYPE':'Education level',\n",
    "    'NAME_FAMILY_STATUS':'Marital status',\n",
    "    'NAME_HOUSING_TYPE':'Dwelling',\n",
    "    'DAYS_BIRTH':'Age',\n",
    "    'DAYS_EMPLOYED': 'Employment length',\n",
    "    'FLAG_MOBIL': 'Has a mobile phone',\n",
    "    'FLAG_WORK_PHONE': 'Has a work phone',\n",
    "    'FLAG_PHONE': 'Has a phone',\n",
    "    'FLAG_EMAIL': 'Has an email',\n",
    "    'OCCUPATION_TYPE': 'Job title',\n",
    "    'CNT_FAM_MEMBERS': 'Family member count',\n",
    "    'Account age': 'Account age'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test\n",
    "def data_split(df, test_size):\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7511a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train_original, cc_test_original = data_split(cc_data_full_data, 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6fec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train_original.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_test_original.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ca907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the dataset so that the original stays untouched\n",
    "cc_train_copy = cc_train_original.copy()\n",
    "cc_test_copy = cc_test_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data_full_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befa5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data_full_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data_full_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a70b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that will return the value count and frequency of each observation within a feature\n",
    "def value_cnt_norm_cal(df,feature):\n",
    "    ftr_value_cnt = df[feature].value_counts()\n",
    "    ftr_value_cnt_norm = df[feature].value_counts(normalize=True) * 100\n",
    "    ftr_value_cnt_concat = pd.concat([ftr_value_cnt, ftr_value_cnt_norm], axis=1)\n",
    "    ftr_value_cnt_concat.columns = ['Count', 'Frequency (%)']\n",
    "    return ftr_value_cnt_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34261384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create display general information about the feature\n",
    "def gen_info_feat(df,feature):\n",
    "    if feature == 'Age':\n",
    "        # change the feature to be express in positive numbers days\n",
    "        print('Description:\\n{}'.format((np.abs(df[feature])/365.25).describe()))\n",
    "        print('*'*50)\n",
    "        print('Object type:{}'.format(df[feature].dtype))\n",
    "    elif feature == 'Employment length':\n",
    "        # select only the rows where the rows are negative to ignore whose who have retired or unemployed\n",
    "        employment_len_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] < 0]\n",
    "        employment_len_no_ret_yrs = np.abs(employment_len_no_ret)/365.25\n",
    "        print('Description:\\n{}'.format((employment_len_no_ret_yrs).describe()))\n",
    "        print('*'*50)\n",
    "        print('Object type:{}'.format(employment_len_no_ret.dtype))\n",
    "    elif feature ==  'Account age':\n",
    "        # change the account age to a positive number of months\n",
    "        print('Description:\\n{}'.format((np.abs(df[feature])).describe()))\n",
    "        print('*'*50)\n",
    "        print('Object type:{}'.format(df[feature].dtype))\n",
    "    else:\n",
    "        print('Description:\\n{}'.format(df[feature].describe()))\n",
    "        print('*'*50)\n",
    "        print('Object type:\\n{}'.format(df[feature].dtype))\n",
    "        print('*'*50)\n",
    "        value_cnt = value_cnt_norm_cal(df,feature)\n",
    "        print('Value count:\\n{}'.format(value_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a10dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a pie chart plot\n",
    "def create_pie_plot(df,feature):\n",
    "    if feature ==  'Dwelling' or 'Education level':\n",
    "        ratio_size = value_cnt_norm_cal(df, feature)\n",
    "        ratio_size_len = len(ratio_size.index)\n",
    "        ratio_list = []\n",
    "        for i in range(ratio_size_len):\n",
    "            ratio_list.append(ratio_size.iloc[i]['Frequency (%)'])\n",
    "        fig, ax = plt.subplots(figsize=(10,10))\n",
    "        # %1.2f%% display decimals in the pie chart with 2 decimal places\n",
    "        plt.pie(ratio_list, startangle=90, wedgeprops={'edgecolor' :'black'})\n",
    "        plt.title('Pie chart of {}'.format(feature))\n",
    "        plt.legend(loc='best',labels=ratio_size.index)\n",
    "        plt.axis('equal')\n",
    "        return plt.show()\n",
    "    else :\n",
    "        ratio_size = value_cnt_norm_cal(df, feature)\n",
    "        ratio_size_len = len(ratio_size.index)\n",
    "        ratio_list = []\n",
    "        for i in range(ratio_size_len):\n",
    "            ratio_list.append(ratio_size.iloc[i]['Frequency (%)'])\n",
    "        fig, ax = plt.subplots(figsize=(10,10))\n",
    "        # %1.2f%% display decimals in the pie chart with 2 decimal places\n",
    "        plt.pie(ratio_list, labels=ratio_size.index, autopct='%1.2f%%', startangle=90, wedgeprops={'edgecolor' :'black'})\n",
    "        plt.title('Pie chart of {}'.format(feature))\n",
    "        plt.legend(loc='best')\n",
    "        plt.axis('equal')\n",
    "        return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cdb829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a bar chart plot\n",
    "def create_bar_plot(df,feature):\n",
    "    if feature == 'Marital status' or 'Dwelling' or 'Job title' or 'Employment status' or 'Education level':\n",
    "        fig, ax = plt.subplots(figsize=(6,10))\n",
    "        sns.barplot(x=value_cnt_norm_cal(df,feature).index,y=value_cnt_norm_cal(df,feature).values[:,0])\n",
    "        ax.set_xticklabels(labels=value_cnt_norm_cal(df,feature).index,rotation=45,ha='right')\n",
    "        plt.xlabel('{}'.format(feature))\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('{} count'.format(feature))\n",
    "        return plt.show()\n",
    "    else :\n",
    "        fig, ax = plt.subplots(figsize=(6,10))\n",
    "        sns.barplot(x=value_cnt_norm_cal(df,feature).index,y=value_cnt_norm_cal(df,feature).values[:,0])\n",
    "        plt.xlabel('{}'.format(feature))\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('{} count'.format(feature))\n",
    "        return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c85772",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train_copy.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd4dd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5583e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51c067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb611a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f7c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed007292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,feat_with_outliers = ['Family member count','Income', 'Employment length']):\n",
    "        self.feat_with_outliers = feat_with_outliers\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if (set(self.feat_with_outliers).issubset(df.columns)):\n",
    "            # 25% quantile\n",
    "            Q1 = df[self.feat_with_outliers].quantile(.25)\n",
    "            # 75% quantile\n",
    "            Q3 = df[self.feat_with_outliers].quantile(.75)\n",
    "            IQR = Q3 - Q1\n",
    "            # keep the data within 1.5 IQR\n",
    "            df = df[~((df[self.feat_with_outliers] < (Q1 - 1.5 * IQR)) |(df[self.feat_with_outliers] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9390f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropFeatures(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,feature_to_drop = ['ID','Has a mobile phone','Children count','Job title']):\n",
    "        self.feature_to_drop = feature_to_drop\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if (set(self.feature_to_drop).issubset(df.columns)):\n",
    "            df.drop(self.feature_to_drop,axis=1,inplace=True)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579db08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeConversionHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feat_with_days = ['Employment length', 'Age'], feat_with_months = ['Account age']):\n",
    "        self.feat_with_days = feat_with_days\n",
    "        self.feat_with_months = feat_with_months\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        if (set(self.feat_with_days).issubset(X.columns)) & (set(self.feat_with_months).issubset(X.columns)):\n",
    "            # convert days to absolute value\n",
    "            X[['Employment length','Age']] = np.abs(X[['Employment length','Age']])\n",
    "            # convert months to absolute value\n",
    "            X['Account age'] = np.abs(X['Account age'])\n",
    "            return X\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6828c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetireeHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, df):\n",
    "        return self\n",
    "    def transform(self, df):\n",
    "        if 'Employment length' in df.columns:\n",
    "            # select rows with employment length is 365243 which corresponds to retirees\n",
    "            df_ret_idx = df['Employment length'][df['Employment length'] == 365243].index\n",
    "            # change 365243 to 0\n",
    "            df.loc[df_ret_idx,'Employment length'] = 0\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Employment length is not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c46125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkewnessHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,feat_with_skewness=['Income','Age']):\n",
    "        self.feat_with_skewness = feat_with_skewness\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if (set(self.feat_with_skewness).issubset(df.columns)):\n",
    "            # Handle skewness with cubic root transformation\n",
    "            df[self.feat_with_skewness] = np.cbrt(df[self.feat_with_skewness])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d70ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinningNumToYN(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,feat_with_num_enc=['Has a work phone','Has a phone','Has an email']):\n",
    "        self.feat_with_num_enc = feat_with_num_enc\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if (set(self.feat_with_num_enc).issubset(df.columns)):\n",
    "            # Change 0 to N and 1 to Y for all the features in feat_with_num_enc\n",
    "            for ft in self.feat_with_num_enc:\n",
    "                df[ft] = df[ft].map({1:'Y',0:'N'})\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotWithFeatNames(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,one_hot_enc_ft = ['Gender', 'Marital status', 'Dwelling', 'Employment status', 'Has a car', 'Has a property', 'Has a work phone', 'Has a phone', 'Has an email']):\n",
    "        self.one_hot_enc_ft = one_hot_enc_ft\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if (set(self.one_hot_enc_ft).issubset(df.columns)):\n",
    "            # function to one hot encode the features in one_hot_enc_ft\n",
    "            def one_hot_enc(df,one_hot_enc_ft):\n",
    "                one_hot_enc = OneHotEncoder()\n",
    "                one_hot_enc.fit(df[one_hot_enc_ft])\n",
    "                # get the result of the one hot encoding columns names\n",
    "                feat_names_one_hot_enc = one_hot_enc.get_feature_names_out(one_hot_enc_ft)\n",
    "                # change the array of the one hot encoding to a dataframe with the column names\n",
    "                df = pd.DataFrame(one_hot_enc.transform(df[self.one_hot_enc_ft]).toarray(),columns=feat_names_one_hot_enc,index=df.index)\n",
    "                return df\n",
    "            # function to concatenat the one hot encoded features with the rest of features that were not encoded\n",
    "            def concat_with_rest(df,one_hot_enc_df,one_hot_enc_ft):\n",
    "                # get the rest of the features\n",
    "                rest_of_features = [ft for ft in df.columns if ft not in one_hot_enc_ft]\n",
    "                # concatenate the rest of the features with the one hot encoded features\n",
    "                df_concat = pd.concat([one_hot_enc_df, df[rest_of_features]],axis=1)\n",
    "                return df_concat\n",
    "            # one hot encoded dataframe\n",
    "            one_hot_enc_df = one_hot_enc(df,self.one_hot_enc_ft)\n",
    "            # returns the concatenated dataframe\n",
    "            full_df_one_hot_enc = concat_with_rest(df,one_hot_enc_df,self.one_hot_enc_ft)\n",
    "            return full_df_one_hot_enc\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec9ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalFeatNames(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,ordinal_enc_ft = ['Education level']):\n",
    "        self.ordinal_enc_ft = ordinal_enc_ft\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if 'Education level' in df.columns:\n",
    "            ordinal_enc = OrdinalEncoder()\n",
    "            df[self.ordinal_enc_ft] = ordinal_enc.fit_transform(df[self.ordinal_enc_ft])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Education level is not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxWithFeatNames(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,min_max_scaler_ft = ['Age', 'Income', 'Account age', 'Employment length']):\n",
    "        self.min_max_scaler_ft = min_max_scaler_ft\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if (set(self.min_max_scaler_ft).issubset(df.columns)):\n",
    "            min_max_enc = MinMaxScaler()\n",
    "            df[self.min_max_scaler_ft] = min_max_enc.fit_transform(df[self.min_max_scaler_ft])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OversampleSMOTE(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if 'Is high risk' in df.columns:\n",
    "            # SMOTE function to oversample the minority class to fix the imbalance data\n",
    "            smote = SMOTE()\n",
    "            X_bal, y_bal = smote.fit_resample(df.loc[:, df.columns != 'Is high risk'],df['Is high risk'].astype('int64'))\n",
    "            df_bal = pd.concat([pd.DataFrame(X_bal),pd.DataFrame(y_bal)],axis=1)\n",
    "            return df_bal\n",
    "        else:\n",
    "            print(\"Is high risk is not in the dataframe\")\n",
    "            return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0650dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42160340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline(df):\n",
    "    min_max_scaler_ft = ['Age', 'Income', 'Account age', 'Employment length']\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('outlier_remover', OutlierRemover()),\n",
    "        ('feature_dropper', DropFeatures()),\n",
    "        ('time_conversion_handler', TimeConversionHandler()),\n",
    "        ('retiree_handler', RetireeHandler()),\n",
    "        ('skewness_handler', SkewnessHandler()),\n",
    "        ('binning_num_to_yn', BinningNumToYN()),\n",
    "        ('one_hot_with_feat_names', OneHotWithFeatNames()),\n",
    "        ('ordinal_feat_names', OrdinalFeatNames()),\n",
    "        ('min_max_with_feat_names', MinMaxWithFeatNames()),\n",
    "        ('oversample_smote', OversampleSMOTE())\n",
    "    ])\n",
    "    df_pipe_prep = pipeline.fit_transform(df)\n",
    "    return df_pipe_prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebcd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # Hide the copy warning\n",
    "cc_train_prep = full_pipeline(cc_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa08a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09416d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train data into X and y (target)\n",
    "X_cc_train_prep, y_cc_train_prep = cc_train_prep.loc[:, cc_train_prep.columns != 'Is high risk'], cc_train_prep['Is high risk'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877cc21b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadddb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cdb566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9d9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5079800c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e20e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data_full_data = pd.read_csv('application_record.csv')\n",
    "credit_status = pd.read_csv('credit_record.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5596ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'sgd':SGDClassifier(random_state=42),\n",
    "    'logistic_regression':LogisticRegression(random_state=42,max_iter=1000),\n",
    "#   'support_vector_machine':SVC(random_state=42,probability=True),\n",
    "    'gaussian_naive_bayes':GaussianNB(),\n",
    "    'k_nearest_neighbors':KNeighborsClassifier(),\n",
    "    'gradient_boosting':GradientBoostingClassifier(random_state=42),\n",
    "    'linear_discriminant_analysis':LinearDiscriminantAnalysis(),\n",
    "    'bagging':BaggingClassifier(random_state=42),\n",
    "    'neural_network':MLPClassifier(random_state=42,max_iter=1000),\n",
    "    'adaboost':AdaBoostClassifier(random_state=42),\n",
    "    'extra_trees':ExtraTreesClassifier(random_state=42),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23270df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_importance_plot(model, model_name):\n",
    "    if model_name not in ['support_vector_machine','gaussian_naive_bayes','k_nearest_neighbors','bagging','neural_network']:\n",
    "        # change xtick font size\n",
    "        plt.rcParams['xtick.labelsize'] = 12\n",
    "        plt.rcParams['ytick.labelsize'] = 12\n",
    "        # top 10 most predictive features\n",
    "        top_10_feat = FeatureImportances(model, relative=False, topn=10)\n",
    "        # top 10 least predictive features\n",
    "        bottom_10_feat = FeatureImportances(model, relative=False, topn=-10)\n",
    "        #change the figure size\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        #change x label font size\n",
    "        plt.xlabel('xlabel', fontsize=14)\n",
    "        # Fit to get the feature importances\n",
    "        top_10_feat.fit(X_cc_train_prep, y_cc_train_prep)\n",
    "        # show the plot\n",
    "        top_10_feat.show()\n",
    "        print('\\n')\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.xlabel('xlabel', fontsize=14)\n",
    "        # Fit to get the feature importances\n",
    "        bottom_10_feat.fit(X_cc_train_prep, y_cc_train_prep)\n",
    "        # show the plot\n",
    "        bottom_10_feat.show()\n",
    "        print('\\n')\n",
    "    else:\n",
    "        print('No feature importance for {0}'.format(model_name))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_prediction_func(model_trn):\n",
    "    # check if y_train_copy_pred exists, if not create it\n",
    "    y_cc_train_pred_path = Path('saved_models/{0}/y_train_copy_pred_{0}.sav'.format(model_name))\n",
    "    try:\n",
    "        y_cc_train_pred_path.resolve(strict=True)\n",
    "    except FileNotFoundError:\n",
    "        #cross validation prediction with kfold = 10\n",
    "        y_cc_train_pred = cross_val_predict(model_trn,X_cc_train_prep,y_cc_train_prep,cv=10,n_jobs=-1)\n",
    "        #save the predictions\n",
    "        joblib.dump(y_cc_train_pred,y_cc_train_pred_path)\n",
    "        return y_cc_train_pred\n",
    "    else:\n",
    "        # if it exist load the predictions\n",
    "        y_cc_train_pred = joblib.load(y_cc_train_pred_path)\n",
    "        return y_cc_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30bd615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_func(model_name):\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    #plot confusion matrix\n",
    "    conf_matrix = ConfusionMatrixDisplay.from_predictions(y_cc_train_prep,y_prediction_func(model_trn),ax=ax, cmap='Blues',values_format='d')\n",
    "    # remove the grid\n",
    "    plt.grid(b=None)\n",
    "    # increase the font size of the x and y labels\n",
    "    plt.xlabel('Predicted label', fontsize=14)\n",
    "    plt.ylabel('True label', fontsize=14)\n",
    "    #give a title to the plot using the model name\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    #show the plot\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_func(model_trn,model_name):\n",
    "    # check if y probabilities file exists, if not create it\n",
    "    y_proba_path = Path('saved_models/{0}/y_cc_train_proba_{0}.sav'.format(model_name))\n",
    "    try:\n",
    "        y_proba_path.resolve(strict=True)\n",
    "    except FileNotFoundError:\n",
    "        y_cc_train_proba = model_trn.predict_proba(X_cc_train_prep)\n",
    "        joblib.dump(y_cc_train_proba,y_proba_path)\n",
    "    else:\n",
    "        # if path exist load the y probabilities file\n",
    "        y_cc_train_proba = joblib.load(y_proba_path)\n",
    "    skplt.metrics.plot_roc_curve(y_cc_train_prep, y_cc_train_proba, title = 'ROC curve for {0}'.format(model_name), cmap='cool',figsize=(8,6), text_fontsize='large')\n",
    "    #remove the gride\n",
    "    plt.grid(b=None)\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c02236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func(model_trn, model_name):\n",
    "    # check if score file exists, if not create it\n",
    "    class_report_path = Path('saved_models/{0}/class_report_{0}.sav'.format(model_name))\n",
    "    try:\n",
    "        class_report_path.resolve(strict=True)\n",
    "    except FileNotFoundError:\n",
    "        # calculate the scores of the model\n",
    "        class_report = classification_report(y_cc_train_prep,y_prediction_func(model_trn))\n",
    "        print(class_report)\n",
    "        # save the scores\n",
    "        joblib.dump(class_report,class_report_path)\n",
    "    else:\n",
    "        # if it exist load the scores\n",
    "        class_report = joblib.load(class_report_path)\n",
    "        print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,model_name):\n",
    "    # check if the model file exist and if not create, train and save it\n",
    "    model_file_path = Path('saved_models/{0}/{0}_model.sav'.format(model_name))\n",
    "    try:\n",
    "        model_file_path.resolve(strict=True)\n",
    "    except FileNotFoundError:\n",
    "        if model_name == 'sgd':\n",
    "            # for sgd, loss = 'hinge' does not have a predict_proba method. Therefore, we use a calibrated model\n",
    "            calibrated_model = CalibratedClassifierCV(model, cv=10, method='sigmoid')\n",
    "            model_trn = calibrated_model.fit(X_cc_train_prep,y_cc_train_prep)\n",
    "        else:\n",
    "            model_trn = model.fit(X_cc_train_prep,y_cc_train_prep)\n",
    "        joblib.dump(model_trn,model_file_path)\n",
    "        # plot the most and least predictive features\n",
    "        return model_trn\n",
    "    else:\n",
    "        # if path exist load the model\n",
    "        model_trn = joblib.load(model_file_path)\n",
    "        # plot the most and least predictive features\n",
    "        return model_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_check():\n",
    "    # check if the folder for saving the model exists, if not create it\n",
    "    if not os.path.exists('saved_models/{}'.format(model_name)):\n",
    "        os.makedirs('saved_models/{}'.format(model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name,model in classifiers.items():\n",
    "    # title formatting\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    print('  {}  '.center(50,'-').format(model_name))\n",
    "    print('\\n')\n",
    "    # check if the folder for saving the model exists, if not create it\n",
    "    folder_check()\n",
    "    # train the model\n",
    "    model_trn = train_model(model,model_name)\n",
    "    # print the scores from the classification report\n",
    "    score_func(model_trn, model_name)\n",
    "    # plot the ROC curve\n",
    "    roc_curve_func(model_trn,model_name)\n",
    "    # plot the confusion matrix\n",
    "    confusion_matrix_func(model_name)\n",
    "    # print the most and least predictive features\n",
    "    feat_importance_plot(model,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5f4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05569bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb32f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a43ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
